{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting bbbc021 data. Run this script first just to load the data. I'd like to put all this into a simple plug-and-play, but the process of loading data is just too fiddly to easily put into a fuss-free UI.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import collections as coll\n",
    "import re\n",
    "#import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "#import toolz as tz\n",
    "#from microscopium.screens import image_xpress\n",
    "from skimage import io, img_as_float\n",
    "from skimage.transform import downscale_local_mean\n",
    "from sklearn.utils.extmath import cartesian as skcartesian\n",
    "\n",
    "import skynet.bbbc021io as xio\n",
    "import skynet.patch_extraction as pex\n",
    "import skynet.utils as utils\n",
    "\n",
    "data_path = \"/Users/don/Documents/patch_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load a db of plate-well, compound, concentration and moa\n",
    "#Note that this database includes NaNs\n",
    "\n",
    "labels_db = pd.read_csv(\"/Users/don/Documents/hcs/label_db2.csv\",\n",
    "                       usecols=[1,2,3,4])\n",
    "#Don't load the first column; it's just a col of indices\n",
    "#If the compound-concentration of some plate-well didn't have an moa,\n",
    "#it won't be in this database\n",
    "\n",
    "# Sample search\n",
    "# result = xio.search_labels('BBBC021-40111-B03', labels_db)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>Extracting Image data</font><br><br>\n",
    "The main folder, 'BBBC021', has a bunch of subfolders, e.g. 'Week3_xxxx'.<br>\n",
    "The platenumber is found in the subfolder name; that's the 'xxxx'.<br>\n",
    "Each subfolder has a bunch of images; the well number is in the image name. <br><br>\n",
    "Desired output:\n",
    " - A list of patches, each row comprises of 1 patch and its labels: [array:(20 x 20 x 3), plate-num, cc-label, moa-label]\n",
    " - Let's not unravel the patch yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 2560, 3)\n",
      "No. of grids =  357\n"
     ]
    }
   ],
   "source": [
    "#Declare some params:\n",
    "n_patches = 100\n",
    "patch_len = 120\n",
    "ds_factor = (3,3,1)\n",
    "\n",
    "# main_list is a list of all subfolder names\n",
    "data_path = '/Users/don/Documents/patch_data/'\n",
    "path = '/Users/don/Documents/BBBC021'\n",
    "main_list = xio.get_main_list(path)\n",
    "\n",
    "#csv_path = '/Users/don/Documents/BBBC021/BBBC021_parsed_metadata.csv'\n",
    "#treatments = xio.get_labels_from_csv(csv_path, verbose=True)\n",
    "\n",
    "\n",
    "arbitrary_img_path = path+\"/Week1_22123/Week1_150607_B02_s1_w107447158-AC76-4844-8431-E6A954BD1174.jpeg\"\n",
    "test_img = io.imread(arbitrary_img_path)\n",
    "im_h, im_w, ch = test_img.shape\n",
    "print(test_img.shape)\n",
    "\n",
    "print(\"No. of grids = \",pex.count_grids(patch_len, im_h, im_w))\n",
    "#plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 55 subfolders...60 images found in subfolder\n",
      "20 images retrieved. Runtime = 29.48s\n",
      "2 of 55 subfolders...60 images found in subfolder\n",
      "20 images retrieved. Runtime = 31.63s\n",
      "3 of 55 subfolders...60 images found in subfolder\n",
      "20 images retrieved. Runtime = 30.40s\n",
      "4 of 55 subfolders...60 images found in subfolder\n",
      "25 images retrieved. Runtime = 29.88s\n",
      "5 of 55 subfolders...60 images found in subfolder\n",
      "25 images retrieved. Runtime = 28.09s\n",
      "6 of 55 subfolders...60 images found in subfolder\n",
      "25 images retrieved. Runtime = 29.00s\n",
      "7 of 55 subfolders...60 images found in subfolder\n",
      "18 images retrieved. Runtime = 31.09s\n",
      "8 of 55 subfolders...60 images found in subfolder\n",
      "18 images retrieved. Runtime = 28.35s\n",
      "9 of 55 subfolders...60 images found in subfolder\n",
      "18 images retrieved. Runtime = 30.01s\n",
      "10 of 55 subfolders...60 images found in subfolder\n",
      "16 images retrieved. Runtime = 30.70s\n",
      "11 of 55 subfolders...60 images found in subfolder\n",
      "16 images retrieved. Runtime = 30.10s\n",
      "12 of 55 subfolders...60 images found in subfolder\n",
      "16 images retrieved. Runtime = 29.63s\n",
      "13 of 55 subfolders...60 images found in subfolder\n",
      "14 images retrieved. Runtime = 28.36s\n",
      "14 of 55 subfolders...60 images found in subfolder\n",
      "14 images retrieved. Runtime = 28.01s\n",
      "15 of 55 subfolders...60 images found in subfolder\n",
      "14 images retrieved. Runtime = 27.84s\n",
      "16 of 55 subfolders...60 images found in subfolder\n",
      "32 images retrieved. Runtime = 29.23s\n",
      "17 of 55 subfolders...60 images found in subfolder\n",
      "32 images retrieved. Runtime = 29.87s\n",
      "18 of 55 subfolders...60 images found in subfolder\n",
      "32 images retrieved. Runtime = 29.36s\n",
      "19 of 55 subfolders...60 images found in subfolder\n",
      "21 images retrieved. Runtime = 30.21s\n",
      "20 of 55 subfolders...60 images found in subfolder\n",
      "21 images retrieved. Runtime = 28.54s\n",
      "21 of 55 subfolders...60 images found in subfolder\n",
      "21 images retrieved. Runtime = 30.17s\n",
      "22 of 55 subfolders...60 images found in subfolder\n",
      "14 images retrieved. Runtime = 26.96s\n",
      "23 of 55 subfolders...60 images found in subfolder\n",
      "14 images retrieved. Runtime = 27.11s\n",
      "24 of 55 subfolders...60 images found in subfolder\n",
      "14 images retrieved. Runtime = 26.84s\n",
      "25 of 55 subfolders...60 images found in subfolder\n",
      "22 images retrieved. Runtime = 27.41s\n",
      "26 of 55 subfolders...60 images found in subfolder\n",
      "22 images retrieved. Runtime = 27.32s\n",
      "27 of 55 subfolders...60 images found in subfolder\n",
      "22 images retrieved. Runtime = 27.32s\n",
      "28 of 55 subfolders...60 images found in subfolder\n",
      "15 images retrieved. Runtime = 27.11s\n",
      "29 of 55 subfolders...60 images found in subfolder\n",
      "15 images retrieved. Runtime = 27.27s\n",
      "30 of 55 subfolders...60 images found in subfolder\n",
      "15 images retrieved. Runtime = 27.29s\n",
      "31 of 55 subfolders...60 images found in subfolder\n",
      "12 images retrieved. Runtime = 27.56s\n",
      "32 of 55 subfolders...60 images found in subfolder\n",
      "12 images retrieved. Runtime = 27.85s\n",
      "33 of 55 subfolders...60 images found in subfolder\n",
      "12 images retrieved. Runtime = 27.64s\n",
      "34 of 55 subfolders...60 images found in subfolder\n",
      "13 images retrieved. Runtime = 26.79s\n",
      "35 of 55 subfolders...60 images found in subfolder\n",
      "13 images retrieved. Runtime = 27.07s\n",
      "36 of 55 subfolders...60 images found in subfolder\n",
      "13 images retrieved. Runtime = 26.52s\n",
      "37 of 55 subfolders...60 images found in subfolder\n",
      "12 images retrieved. Runtime = 32.30s\n",
      "38 of 55 subfolders...60 images found in subfolder\n",
      "12 images retrieved. Runtime = 30.41s\n",
      "39 of 55 subfolders...60 images found in subfolder\n",
      "12 images retrieved. Runtime = 28.49s\n",
      "40 of 55 subfolders...60 images found in subfolder\n",
      "17 images retrieved. Runtime = 28.59s\n",
      "41 of 55 subfolders...60 images found in subfolder\n",
      "17 images retrieved. Runtime = 27.85s\n",
      "42 of 55 subfolders...60 images found in subfolder\n",
      "17 images retrieved. Runtime = 32.71s\n",
      "43 of 55 subfolders...60 images found in subfolder\n",
      "17 images retrieved. Runtime = 27.68s\n",
      "44 of 55 subfolders...60 images found in subfolder\n",
      "17 images retrieved. Runtime = 27.76s\n",
      "45 of 55 subfolders...60 images found in subfolder\n",
      "15 images retrieved. Runtime = 29.59s\n",
      "46 of 55 subfolders...60 images found in subfolder\n",
      "15 images retrieved. Runtime = 30.13s\n",
      "47 of 55 subfolders...60 images found in subfolder\n",
      "15 images retrieved. Runtime = 32.42s\n",
      "48 of 55 subfolders...60 images found in subfolder\n",
      "14 images retrieved. Runtime = 28.56s\n",
      "49 of 55 subfolders...60 images found in subfolder\n",
      "14 images retrieved. Runtime = 28.99s\n",
      "50 of 55 subfolders...60 images found in subfolder\n",
      "15 images retrieved. Runtime = 28.37s\n",
      "51 of 55 subfolders...60 images found in subfolder\n",
      "15 images retrieved. Runtime = 26.61s\n",
      "52 of 55 subfolders...60 images found in subfolder\n",
      "15 images retrieved. Runtime = 26.71s\n",
      "53 of 55 subfolders...60 images found in subfolder\n",
      "19 images retrieved. Runtime = 29.61s\n",
      "54 of 55 subfolders...60 images found in subfolder\n",
      "19 images retrieved. Runtime = 28.80s\n",
      "55 of 55 subfolders...60 images found in subfolder\n",
      "19 images retrieved. Runtime = 29.25s\n",
      "\n",
      "All done in 0:26:22\n",
      "962 images used\n"
     ]
    }
   ],
   "source": [
    "#Takes ~20 to 30 mins\n",
    "t0 = time.time()\n",
    "main_data_list = []\n",
    "\n",
    "idx = 1\n",
    "img_counter = 0\n",
    "for sf in main_list:\n",
    "    t_i = time.time()\n",
    "    subpath = path + '/' + sf\n",
    "    print(\"%s of %s subfolders...\" % (idx, len(main_list))\n",
    "          , end=\"\")\n",
    "    sf_data_list_temp = xio.get_subfolder_patch_data(subpath, \n",
    "                                                     n_patches, \n",
    "                                                     patch_len, \n",
    "                                                     labels_db, \n",
    "                                                     ds=ds_factor,\n",
    "                                                     verbose=1)\n",
    "    print(\"%s images retrieved. \" % len(sf_data_list_temp), end=\"\")\n",
    "    img_counter = img_counter + len(sf_data_list_temp)\n",
    "    main_data_list = main_data_list + sf_data_list_temp\n",
    "    idx +=1\n",
    "    print(\"Runtime = %.2fs\" % (time.time() - t_i))\n",
    "\n",
    "data_list = np.array(main_data_list)\n",
    "del main_data_list\n",
    "\n",
    "dt = time.time() - t0\n",
    "m, s = divmod(dt, 60)\n",
    "h, m = divmod(m, 60)\n",
    "print(\"\")\n",
    "print(\"All done in %d:%02d:%02d\" % (h, m, s))\n",
    "print(\"%s images used\" % img_counter)\n",
    "#Maybe remove all subfolder-level comments in get_subfolder_patch_data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5><b>Loading Block</b></font>\n",
    "<br>Load the p_list that you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_list = pickle.load(open(data_path+'d_ls0.p', 'rb'))\n",
    "print(p_list.shape)\n",
    "print(p_list[:,0][0].shape)\n",
    "#p_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5><b>Optional step: Remove Overrepresented Classes</b></font><br> Remove compounds <i>taxol</i> and DMSO, or just DMSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632\n",
      "(100, 40, 40, 3)\n",
      "(632, 6)\n"
     ]
    }
   ],
   "source": [
    "d_list_filtered = []\n",
    "\n",
    "for row in data_list:\n",
    "    if row[2] != 'DMSO':\n",
    "        d_list_filtered.append(row)\n",
    "\n",
    "print(len(d_list_filtered))\n",
    "print(d_list_filtered[0][0].shape)\n",
    "\n",
    "# Add an image index\n",
    "for i in range(len(d_list_filtered)):\n",
    "    new_row = np.append(d_list_filtered[i], i)\n",
    "    d_list_filtered[i] = new_row\n",
    "\n",
    "d_list_filtered = np.array(d_list_filtered)\n",
    "print(d_list_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632\n",
      "632\n",
      "632\n"
     ]
    }
   ],
   "source": [
    "# Check no. of images\n",
    "pw_names = []\n",
    "img_idx_ls = []\n",
    "for i in range(len(d_list_filtered)):\n",
    "    pw_names.append(d_list_filtered[i][1])\n",
    "    img_idx_ls.append(d_list_filtered[i][5])\n",
    "\n",
    "print(len(d_list_filtered))\n",
    "print(len(list(set(pw_names))))\n",
    "print(len(list(set(img_idx_ls))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5><b>Equalize Class Proportions at Img Level</b></font><br>This is the first randomization step. I was hoping to be able to save all image data, but I don't think we can k-fold anything, because the smallest class has too few images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of classes = 12\n",
      "Class sizes:\n",
      "0: 15\n",
      "1: 36\n",
      "2: 18\n",
      "3: 27\n",
      "4: 24\n",
      "5: 36\n",
      "6: 22\n",
      "7: 10\n",
      "8: 42\n",
      "9: 357\n",
      "10: 21\n",
      "11: 24\n",
      "size of smallest class = 10\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "# Get the moa labels, moa_ls\n",
    "moa_ls = []\n",
    "for img in d_list_filtered:\n",
    "    moa_ls.append(img[4])\n",
    "moa_ls = np.array(moa_ls)\n",
    "\n",
    "# Get the different class names: moa_labels\n",
    "moa_labels = list(set(moa_ls))\n",
    "moa_labels.sort()\n",
    "print(\"No. of classes = %s\" % len(moa_labels))\n",
    "\n",
    "# Get the indices of each class\n",
    "# idx is a list of lists s.t. idx[i] = list of the indices of samples in class i\n",
    "class_idx = []\n",
    "for i in range(len(moa_labels)):\n",
    "    class_idx.append(np.where(moa_ls==moa_labels[i])[0])\n",
    "\n",
    "print(\"Class sizes:\")\n",
    "for i in range(len(class_idx)):\n",
    "    print(\"%s: %s\" % (i, len(class_idx[i])))\n",
    "\n",
    "df = np.zeros(len(moa_labels))\n",
    "for i in range(len(df)):\n",
    "    df[i] = len(class_idx[i])\n",
    "nb_small = int(min(df))\n",
    "print(\"size of smallest class = %s\" % nb_small)\n",
    "\n",
    "# For each of the other classes, choose min_class observations at random\n",
    "# sample is a list of the chosen indices\n",
    "# Note that when it comes to the actual smallest class, choosing, say, 10 samples\n",
    "# out of 10 possible choices will just return the whole class\n",
    "sample = []\n",
    "for i in range(len(class_idx)):\n",
    "    subsample = np.random.choice(class_idx[i], nb_small, replace=False)\n",
    "    sample.append(subsample)\n",
    "sample = np.array(sample)\n",
    "sample = sample.flatten()\n",
    "print(len(sample))\n",
    "print(len(list(set(sample))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 6)\n",
      "(100, 40, 40, 3)\n"
     ]
    }
   ],
   "source": [
    "d_list2 = d_list_filtered[sample]\n",
    "print(d_list2.shape)\n",
    "print(d_list2[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "# Check no. of images, number of pw_names, no. of img_idx's\n",
    "pw_names = []\n",
    "img_idx_ls = []\n",
    "for i in range(len(d_list2)):\n",
    "    pw_names.append(d_list2[i][1])\n",
    "    img_idx_ls.append(d_list2[i][5])\n",
    "\n",
    "print(len(d_list2))\n",
    "print(len(list(set(pw_names))))\n",
    "print(len(list(set(img_idx_ls))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4><b>Data Formatting</b></font><br>\n",
    "Change from image-level to patch-level format. Downscale if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 6)\n",
      "(6,)\n",
      "(40, 40, 3)\n"
     ]
    }
   ],
   "source": [
    "p_list = []\n",
    "\n",
    "for row in d_list2:\n",
    "    patches = row[0]\n",
    "    for patch in patches:\n",
    "        record = [patch, row[1], row[2], row[3], row[4], row[5]]\n",
    "        p_list.append(record)\n",
    "\n",
    "p_list=np.array(p_list)\n",
    "print(p_list.shape)\n",
    "print(p_list[0].shape)\n",
    "print(p_list[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 6)\n"
     ]
    }
   ],
   "source": [
    "colnames = ['img', 'pw', 'cpd', 'cc', 'moa', 'img_idx']\n",
    "df = pd.DataFrame(p_list)\n",
    "df.columns = colnames\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(df, open(data_path+'p2.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5><b>Save Images/Patches to a Folder for Viz</b></font><br>\n",
    "Let's make a quilt too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "short_forms = {'Actin disruptors':'actin_d',\n",
    "              'Aurora kinase inhibitors': 'ak_inhib',\n",
    "              'Cholesterol-lowering': 'chol_l',\n",
    "              'DNA damage': 'dna_dmg',\n",
    "              'DNA replication': 'dna_repl',\n",
    "              'Eg5 inhibitors': 'eg5',\n",
    "              'Epithelial': 'epi',\n",
    "              'Kinase inhibitors': 'k_inhib',\n",
    "              'Microtubule destabilizers': 'mt_dest',\n",
    "              'Microtubule stabilizers': 'mt_stab',\n",
    "              'Protein degradation': 'prot_deg',\n",
    "              'Protein synthesis': 'prot_syn',\n",
    "              }\n",
    "\n",
    "img_data = np.array(list(d0['img']))\n",
    "img_idx_labels = list(d0['img_idx'])\n",
    "#moa_labels = list(d0['moa'])\n",
    "img_idx_ref = list(set(img_idx_labels))\n",
    "print(img_data.shape)\n",
    "\n",
    "save_fn = \"/Users/don/Desktop/patches_g10s_r1/\"\n",
    "quilts = []\n",
    "quilt_w = 10\n",
    "n_patches = 80\n",
    "p_len = 50\n",
    "n_quilt_rows = int(n_patches/quilt_w)\n",
    "\n",
    "for idx in img_idx_ref:\n",
    "    d_temp = d0.loc[d0['img_idx'] == idx]\n",
    "    img_data = np.array(list(d_temp['img']))\n",
    "    moa = list(d_temp['moa'])[0]\n",
    "    moa_s = short_forms[moa]\n",
    "    pw = list(d_temp['pw'])[0]\n",
    "    pw = pw[8:]\n",
    "    cpd = list(d_temp['cpd'])[0]\n",
    "    cc = list(d_temp['cc'])[0]\n",
    "    if cpd == 'mevinolin/lovastatin':\n",
    "        cpd = 'mevinolin.lovastatin'\n",
    "    cpd_cc = str(cpd)+str(cc)\n",
    "    \n",
    "    grid = []\n",
    "    for j in range(0, n_patches, quilt_w):\n",
    "        row = img_data[j:j+quilt_w]\n",
    "        row = np.vstack(row)\n",
    "        grid.append(row)\n",
    "    \n",
    "    grid = np.hstack(np.array(grid))\n",
    "    img_fn = moa_s+'_'+cpd_cc+'_'+pw+'_'+str(idx)+'.png'\n",
    "    #print(img_fn)\n",
    "    save_str = save_fn+img_fn\n",
    "    io.imsave(save_str, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_temp = d0.loc[d0['img_idx'] == 515]\n",
    "print(d_temp.shape)\n",
    "cpd = list(d_temp['pw'])[0]\n",
    "cpd = cpd[8:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4><b>Equalize Class Proportions</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get equal representation of classes at MOA/MOAx level\n",
    "# First, let's see what the class sizes are like from a p_list\n",
    "\n",
    "labels = []\n",
    "for row in p_list:\n",
    "    labels.append(row[3])\n",
    "labels = np.array(labels)\n",
    "labels_ref = list(set(labels))\n",
    "labels_ref.sort()\n",
    "nb_classes = len(labels_ref)\n",
    "print(\"No. of classes = %s\" % nb_classes)\n",
    "\n",
    "#Get a DF of the labels\n",
    "df = []\n",
    "for i in range(len(labels_ref)):\n",
    "    df.append(list(labels).count(labels_ref[i]))\n",
    "\n",
    "df = np.array(df)\n",
    "df_n = df/sum(df) * 100\n",
    "min_class = min(df)\n",
    "print(\"Size of smallest class = %s\" % min_class)\n",
    "print(df_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = []\n",
    "for i in range(len(labels_ref)):\n",
    "    idx.append(np.where(labels==labels_ref[i])[0])\n",
    "\n",
    "# For each of the other classes, choose min_class observations at random\n",
    "sample = []\n",
    "for i in range(len(idx)):\n",
    "    subsample = np.random.choice(idx[i], min_class)\n",
    "    sample.append(subsample)\n",
    "sample = np.array(sample)\n",
    "sample = sample.flatten()\n",
    "\n",
    "# Extract the final p_list\n",
    "p_list_eq = []\n",
    "for i in range(len(sample)):\n",
    "    p_list_eq.append(p_list[sample[i]])\n",
    "\n",
    "print(len(p_list_eq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(p_list_eq, open(data_path+'p_list_g6s_eq.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_patches = p_list_x.shape[0]\n",
    "batch = int(n_patches/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(p_list_x[:batch], open(data_path+'p_list_f3_p1.p', 'wb'))\n",
    "pickle.dump(p_list_x[batch:(2*batch)], open(data_path+'p_list_f3_p2.p', 'wb'))\n",
    "pickle.dump(p_list_x[(2*batch):(3*batch)], open(data_path+'p_list_f3_p3.p', 'wb'))\n",
    "pickle.dump(p_list_x[(3*batch):(4*batch)], open(data_path+'p_list_f3_p4.p', 'wb'))\n",
    "pickle.dump(p_list_x[(4*batch):], open(data_path+'p_list_f3_p5.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_list = pickle.load(open(data_path+'p_list_g8.p', 'rb'))\n",
    "print(p_list.shape)\n",
    "print(p_list[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's cut that up into 4 p_lists, to pickle.dump individually.\n",
    "n_records = len(p_list)\n",
    "idx1 = int(n_records / 4)\n",
    "idx2 = idx1 * 2\n",
    "idx3 = idx1 * 3\n",
    "\n",
    "subset1 = p_list[:idx1]\n",
    "subset2 = p_list[idx1:idx2]\n",
    "subset3 = p_list[idx2:idx3]\n",
    "subset4 = p_list[idx3:]\n",
    "\n",
    "del p_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save them individually\n",
    "pickle.dump(subset1, open('subset1.p', 'wb'))\n",
    "print(\"subset1 pickled.dumped.\")\n",
    "pickle.dump(subset2, open('subset2.p', 'wb'))\n",
    "print(\"subset2 pickled.dumped.\")\n",
    "pickle.dump(subset3, open('subset3.p', 'wb'))\n",
    "print(\"subset3 pickled.dumped.\")\n",
    "pickle.dump(subset4, open('subset4.p', 'wb'))\n",
    "print(\"subset4 pickled.dumped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.save('p_list2', p_list)\n",
    "pickle.dump(p_list, open('p_list0.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image-level data:\n",
    "- data_list.p: 10 (30, 30) patches per image\n",
    "\n",
    "Patch-level data:\n",
    "- p_list: 50 (30, 30, 3) patches, 1.25Gb\n",
    "- p_list0: 10 (30, 30, 3) patches, 285Mb\n",
    "- p_list2: 100 (30, 30, 3) patches, 2.08Gb\n",
    "\n",
    "'f<x>' suffix indicates that DMSO and taxol compounds are removed.\n",
    "- p_list_f0: 300 (30, 30, 3) patches \n",
    "- p_list_f1: 100 (30, 30, 3) patches\n",
    "- p_list_f2: 50 (30, 30, 3) patches\n",
    "- p_list_f3_p1 to ..._p5: 50 (100, 100, 3) patches\n",
    "\n",
    "'g' suffix indicates that only DMSO has been removed. \n",
    "- p_list_g0: 10 (100, 100, 3) patches\n",
    "- p_list_g1: 10 (300, 300, 3) patches\n",
    "- p_list_g2: 100 (30, 30, 3) patches\n",
    "- p_list_g4: 10 (400, 400, 3) patches\n",
    "- p_list_g5_p1 to ..._p5: 50 (100, 100, 3) patches\n",
    "- p_list_g0s_eq: length=12000, shape(50, 50, 3), i.e. 1000 patches per class. Mapping to the image level will not really be possible now, since equalizing class proportions was done at patch level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_list = pickle.load(open('p_list3.p', 'rb'))\n",
    "print(p_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is currently in the format of an np array, shape(n_images, 5)\n",
    "- mydata[0]: all n_patches patches extracted from each image. Shape (n_patches, patchlen, patchlen, 3)\n",
    "- mydata[1]: plate-well coords of image\n",
    "- mydata[2]: compound\n",
    "- mydata[3]: concentration\n",
    "- mydata[4]: moa\n",
    "\n",
    "Now let's partition into training and testing data, according to a ratio specified above. Desired output: a list of length 4:\n",
    "- list[0]: training patches\n",
    "- list[1]: moa label of training patches\n",
    "- list[2]: testing patches\n",
    "- list[3]: moa label of testing patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
